{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6273797",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69471020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "avaliable_year = [\n",
    "    1990,\n",
    "    1988,\n",
    "    1987,\n",
    "    1986,\n",
    "    1985,\n",
    "    1984,\n",
    "    1983,\n",
    "]\n",
    "\n",
    "\n",
    "def scrape_publications(year_list: list[int]) -> list[dict[str, str]]:\n",
    "    \"\"\"Scrape publication details including all metadata from all pages.\n",
    "\n",
    "    Args:\n",
    "        year_list (list[int]): List of years to crawl\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: List of publication dictionaries with complete metadata\n",
    "\n",
    "    \"\"\"\n",
    "    publications = []\n",
    "    base_url = (\n",
    "        \"https://www.hkiaps.cuhk.edu.hk/chks-publications/?current_page=1&filterYear=\"\n",
    "    )\n",
    "\n",
    "    for item in year_list:\n",
    "        url = f\"{base_url}{item}\"\n",
    "        print(f\"Scraping publications for year: {item}\")\n",
    "        try:\n",
    "            response = httpx.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            containers = soup.find_all(\"div\", class_=\"bg-fafafa\")\n",
    "\n",
    "            for container in containers:\n",
    "                pub = {}\n",
    "                headlines = container.find_all(\"div\", class_=\"Headline3\")\n",
    "                if len(headlines) >= 2:\n",
    "                    pub[\"id\"] = headlines[0].get_text(strip=True)\n",
    "                    pub[\"title\"] = headlines[1].get_text(strip=True)\n",
    "                buttons = container.find_all(\"a\")\n",
    "                for button in buttons:\n",
    "                    href = button.get(\"href\", \"\")\n",
    "                    text = button.get_text(strip=True)\n",
    "                    if \"Abstract\" in text:\n",
    "                        pub[\"abstract_url\"] = (\n",
    "                            href\n",
    "                            if href.startswith(\"http\")\n",
    "                            else f\"https://www.hkiaps.cuhk.edu.hk{href}\"\n",
    "                        )\n",
    "                    elif \"Table of Contents\" in text and href.endswith(\".pdf\"):\n",
    "                        pub[\"toc_pdf\"] = href\n",
    "                    elif \"PDF\" in text and href.endswith(\".pdf\"):\n",
    "                        pub[\"pdf_url\"] = href\n",
    "                info_div = container.find(\"div\", class_=\"Body2\")\n",
    "                if info_div:\n",
    "                    pub[\"metadata\"] = info_div.get_text(separator=\" \", strip=True)\n",
    "                img = container.find(\"img\", class_=\"publicationsImg\")\n",
    "                if img:\n",
    "                    pub[\"image_url\"] = img.get(\"src\", \"\")\n",
    "                publications.append(pub)\n",
    "                print(f\"  - {pub.get('id', 'N/A')}: {pub.get('title', 'N/A')[:50]}...\")\n",
    "\n",
    "            print(f\"Found {len(containers)} publications on year {item}.\\n\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on year {item}: {e}\")\n",
    "\n",
    "    return publications\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting scraper...\\n\")\n",
    "    publications = scrape_publications(avaliable_year)  # Adjust total_pages as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1114e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(\n",
    "    publications: list[dict[str, str]],\n",
    "    dir_name: str,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"Save publications to CSV file.\n",
    "\n",
    "    Args:\n",
    "        publications (List[Dict[str, str]]): List of publication dictionaries\n",
    "        filename (str): Output CSV filename\n",
    "\n",
    "    Returns:\n",
    "        None: Writes data to CSV file\n",
    "\n",
    "    \"\"\"\n",
    "    if not publications:\n",
    "        print(\"No publications to save\")\n",
    "        return\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"metadata\",\n",
    "        \"pdf_url\",\n",
    "        \"toc_pdf\",\n",
    "        \"abstract_url\",\n",
    "        \"image_url\",\n",
    "    ]\n",
    "    filepath = Path(dir_name) / filename\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with Path(filepath).open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        writer.writeheader()\n",
    "        writer.writerows(publications)\n",
    "    print(f\"\\nSaved {len(publications)} publications to {filename}\")\n",
    "\n",
    "\n",
    "save_to_csv(\n",
    "    publications, dir_name=\"chks-publications\", filename=\"hkiaps_chks_publications.csv\"\n",
    ")\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total publications: {len(publications)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkiaps-cuhk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
